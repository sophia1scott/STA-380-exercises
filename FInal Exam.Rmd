---
title: "STA 380 Final Exam Exercises"
author: "Sophia Scott & Casey Copland"
date: "8/9/2021"
output: pdf_document
---

```{r, echo=FALSE}
rm(list = ls())
```

# 1: Visual Story telling part 1: Green Buildings

## Part 1 
1. read in file from github and obtain general information on the dataframe, specifically rent prices $/sqft

Rent (all buildings)
Confidence Interval (28.08600 28.75114)
Min - 2.98
1st Q - 19.50
Median - 25.16
Mean - 28.42
3rd Q - 34.18
Max - 250 


2. Make a subset of only green buildings to observe the mean rent rate and it's confidence interval.

There are 685 green building out of 7894 total buildings ... 8.7% are green. 

Confidence Interval (29.04623 to  30.98583) 
mean green buildings: 30.016.

We see the distributions of all rent and green building rent are not normally distributed. It is heavily skewed to the left due to a few very large rent outliers. 
```{r, echo=FALSE}
library(RCurl)
green <- read.csv('greenbuildings.csv')
green <- na.omit(green)
attach(green)

library(mosaic)
summary(green)

#ALL RENT
hist(green$Rent, 25, main = 'All Buildings Rent')
mean(green$Rent)

# confidence interval for sample mean
xbar = mean(green$Rent)
sig_hat = sd(green$Rent)
se_hat = sig_hat/sqrt(nrow(green))
xbar + c(-1.96,1.96)*se_hat

#GREEN ONLY
# Extract buildings with green ratings
green_only = subset(green, green_rating==1)
dim(green_only) #685 green building out of 7894 ... 8.7%

# Not a normal distribution
hist(green_only$Rent, 25, main = 'Green Buildings Rent')
mean(green_only$Rent)

# confidence interval for sample mean
xbar = mean(green_only$Rent)
sig_hat = sd(green_only$Rent)
se_hat = sig_hat/sqrt(nrow(green_only))
xbar + c(-1.96,1.96)*se_hat


```

3. To create a normal distribution we will bootstrap our rent data. This accounts for the outliers and will hopefully create a more normal, centered distribution, thus make accurate population predictions:

Values from bootstrapped normally distributed models:
All Buildings 
95% Confidence Interval (28.0823	28.75703)
Standard Deviation: 0.1743105
mean estimate: 28.41857

Green Buildings
95% Confidence Interval (29.04623, 30.98583)
Standard Deviation = 0.482954
mean estimate: 30.01603

Interesting insight with the standard deviation differences: could be letting us know the majority of the outliers are green building because of the high estimation error, or could be do to what a small subset green buildings are of the total building observations. 

```{r, echo=FALSE}
### Compare with bootstrapping
library(mosaic)
library(FNN)

#ALL BUILDINGS
#bootstrap 2500 times to get a normally distributed mean rent 
boot0 = do(2500)*{
  mean(resample(green)$Rent)
}
head(boot0)
hist(boot0$result, 30, main = 'All Buildings Rent')

#standard deviation
sd(boot0$result)

# Extract the confidence interval from the bootstrapped samples
confint(boot0, level=0.95)
xbar + c(-1.96,1.96)*se_hat


#GREEN ONLY BUILDINGS
#bootstrap 2500 times to get a normally distributed mean rent 
boot1 = do(2500)*{
  mean(resample(green_only)$Rent)
}
head(boot1)
hist(boot1$result, 30, main = 'Green Buildings Rent')

#standard deviation
sd(boot1$result)

# Extract the confidence interval from the bootstrapped samples
confint(boot1, level=0.95)
xbar + c(-1.96,1.96)*se_hat
```

In our CI the rent premium for green buildings is actually $1.59746, not $2.60 as the staff member predicted. 
The additional revenue predicted by our model is $399365 
```{r, include=TRUE}
green_rent_premium = 30.01603 - 28.41857
green_rent_premium 

addition_revenue = green_rent_premium * 250000
addition_revenue
```


The excel guru chose to use median rent price. He chose median because it is robust to outliers, however we do not need our model to be robust to outliers, we need it to be an accurate predictor of the complex rent market. 

The following is a bootstrap histogram of green building rent median. It is not a normal distribution whatsoever and is therefore not be a good predictor of the population, or real world returns on the real-estate developers green building.
```{r, echo=FALSE}
####
# Bootstrap the median
####

median(green_only$Rent)
# Now repeat 2500 times
boot2 = do(2500)*{
  median(resample(green_only)$Rent)
}
head(boot2)

hist(boot2$result, 30)

# But we still get a confidence interval
confint(boot2)
```

## Part 2 
He also remove outliers from another variable column, but did not consider any other variables provided. This deletion seems like a one-off and possibly leading to more inaccurate results, as he never tested for variable importance within the whole system. He deleted buildings with low occupancy based 'on a theory', but not based on actual analysis.

We will go ahead and do further analysis to see if other variables are important in predicting rent prices by finding patterns of green buildings and using various logistic regression models to see which variables have the largest impact on rent prices. 

- Relationships with variables & green variables

```{r, echo=FALSE}
hist(green_only$age)
hist(green_only$renovated)
hist(green_only$stories)
hist(green_only$total_dd_07)
```
Logistic Regression Models 
1. all buildings (numerical)
  - a logistic regression model using all numerical variables with a significance level of .05 (removed variables on original log reg model if they were not significant)
  - create train & test dataset
  - standardize co-efficients since they are all measuring in different units 

```{r, echo=FALSE}
library(aod)
library(ggplot2)
library(reghelper)
#SD for each variable in the data set
sapply(green, sd)

#Split data into training (70%) and validation (30%)
dt = sort(sample(nrow(green), nrow(green)*.7))
train<-green[dt,]
test<-green[-dt,] 

#look @ all buildings - only numerical variables
all_num <- glm(Rent ~. -CS_PropertyID-stories-cd_total_07-total_dd_07-cluster-renovated-class_a-class_b-LEED-green_rating-Energystar-amenities, data = train, family = 'gaussian')
summary(all_num)

#standardized 
beta(all_num)

#Stepwise Logistic Regression
mylogit = step(all_num)

#test accuracy 
#Prediction - need help hear
pred = predict(mylogit,test, type = "response")
finaldata = cbind(test, pred)
```

1.1 all buildings (categorical)
  - a logistic regression model using categorical variables with a significance level of .01 to obtain the odds ratio for those categories 
```{r, echo=FALSE}
#look @ all buildings - only categorical (dummy) variables
all_dummy <- glm(Rent ~ cluster+renovated+class_a+class_b, data = train, family = 'gaussian')
summary(all_dummy)

#Odds Ratio (standardized)
exp(coef(beta(all_dummy)))

```

2. green buildings only (numerical) 
    - a logistic regression model using all numerical variables
```{r, echo=FALSE}

#look @ green buildings - only numerical variables
library(aod)
library(ggplot2)
library(reghelper)
#SD for each variable in the data set
sapply(green, sd)

#Split data into training (70%) and validation (30%)
dt = sort(sample(nrow(green_only), nrow(green_only)*.7))
train<-green_only[dt,]
test<-green_only[-dt,] 

#look @ green buildings - only numerical variables
green_num <- glm(Rent ~. -CS_PropertyID-stories-cd_total_07-total_dd_07-cluster-renovated-class_a-class_b-LEED-green_rating-Energystar-amenities, data = train, family = 'gaussian')
summary(all_num)
```

2.2 green buildings only (categorical)
- a logistic regression model using categorical variables to obtain the odds ratio for those categories 
```{r, echo=FALSE}
#look @ green buildings - only categorical (dummy) variables
green_dummy <- glm(Rent ~ cluster+renovated+class_a+class_b, data = train, family = 'gaussian')
summary(green_dummy)

#odds ratio
exp(coef(beta(green_dummy)))
```
Conclusions: 

When running a logistic regression model on the entire set and then the green only data set, we see the same issue as before of skewed classes. The variables that a statistically significant in the first logistic regression have lower significance levels on the green_only subset. 

The most important variables in predicting rent for all buildings, as well as green buildings was the buildings rent cluster and if it was a class.a building, both variables increased rent prices. This makes sense because a building's local real-estate market would have a high impact on it's price (i.e. a building in downtown austin or a building in college station. You would pay less money in rent to live in college station because it is not a big city where rent prices are high & competitive). Class A also makes sense as a important variable because the nicer quality a building, the higher the rent will be. 



# 2: Visual Story Telling Part 2: Flights at ABIA 

create a figure, or set of related figures, that tell an interesting story about flights into and out of Austin. Provide a clear annotation/caption for each figure, but the figure should be more or less stand-alone

## Flights Figure 1
```{r q2, echo=FALSE}
#libraries used 
library(tidyverse)
library(ggplot2)
library(dplyr)
library(stats)

#read in ABIA dataset 
library(readr)
ABIA <- read_csv("C:/Users/Sophia Scott/Documents/a.STA 380 MACHINE LEARNING/PART 2/ABIA.csv")
View(ABIA)

#remove Nas 
na.omit(ABIA)

```

## Reasons for flight cancelations and which months it happens most? 
```{r q2, echo=FALSE}

#create summary of cancelations grouped by month and category 
cancel_summary = ABIA %>%
  group_by(Month,CancellationCode) %>%
  summarize(count = sum(Cancelled))
#na.omit(cancel_summary) #take out Nas?          

#create plot with summary of cancelation type grouped by months 
cancelation = ggplot(cancel_summary) + 
  geom_col(aes(x=factor(Month), y =count,
               fill = factor(CancellationCode))) + 
  scale_fill_discrete(name = "Cancellation Code", labels = c('carrier', 'weather','security', 'NAS'))

#label the plot
cancelation + labs(x="Months", y="Number of Cancelations",
     title="Flight Cancelations Per Month")


```
Flight Cancelations reasons have been categorized into 4 categories of carrier, weather, security and unknown. 
From this graph we can see that the month of March has the most cancelleations followed by April and February. 
* Carrier Cancelations are the most prominenet in April 
* Weather Cancelations are the most prominenet in Septmenber
* Security Cancelations are the most proinenet in January 
The most prominenent reason for delays is due to carrier issues followed by weather and then security 


## Figure 2

##Which days of the week in each month typically have the longest delay times? 
```{r q2 echo = FALSE}

#create summary grouping the days of the week and month
#use summarize at function to apply function to one column
delay_summary = ABIA %>%
  group_by(Month, DayOfWeek) %>%
  summarize_at(vars(DepDelay), funs(mean(., na.rm=TRUE)))
delay_summary

#trying to use this to create labels 
months = c('Jan', 'Feb', 'March', 'April', 'May', 'June', 'July', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec')
days = c('Mon','Tues', 'Wed', "Thur", "Fri", "Sat", "Sun")
vars(months)
names(days)

#create plot where each month is shown with the average delays for each day of the week 
ggplot(delay_summary)+
  geom_col(aes(x= factor(DayOfWeek), y = DepDelay))+
  facet_wrap(~Month) + labs(x="Days of the week", y="Number of Delays",
     title="Average Weekly amount of Delays split per month")

```
This helps to answer if there is a specific pattern each month on which day is the worst to travel in terms of delays. 
The average delay time per day over all the months is Friday. Making this the most likely worst day to travel. 
The month with the highest weekly average delays is December followed by March. 
September and October have the lowest amount of delays. 

# 3: Portfolio Modeling
1. Construct 3 ETF portfolios 

ETF's chosen 
 - SPY - SPDR S&P 500 Trust ETF, (one of the most popular ETFs to trach S&P 500 stocks)
 - FPRO - Fidelity Real Estate Investment ETF, (follow the real estate industry, Fidelity being a large and well known financial advisory for Americans)
 - SHY - iShares 1-3 Year Treasury Bond ETF (low risk treasury bond ETF)
 - QTEC - First Trust NASDAQ-100 Technology Sector Index Fund (technology ETF, chose due to the large growth in tech and increasing control/opportunity for big tech companies)
 - RPAR - RPAR Risk Parity ETF (follows investment activity of global investors and their trends/strategies which would be predictions of what will happen in markets, could possibly improve performance of the overall portfoio)
 
 As for correlation of ETFs, QTEC and SPY are highly correlated and this makes sense as they follow the 2 largest stock exchanges in the US which would have similar trends to the overall US economy. 
```{r, echo=FALSE}
#the set up 
library(ggstance)
library(mosaic)
library(quantmod)
library(foreach)

# Import a few stocks
mystocks = c( "SPY", "FPRO", "SHY", "QTEC", 'RPAR')

#price data for 5 years
getSymbols(mystocks, from='2016-01-01')
for(ticker in mystocks){
  expr = paste0(ticker, "a=adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

returns = cbind(ClCl(SPYa),
                ClCl(FPROa),
                ClCl(SHYa),
                ClCl(QTECa),
                ClCl(RPARa))

#Remove na values
returns = as.matrix(na.omit(returns))

#correlation between chosen stocks 
cor(returns)
```
Going along with the correlation of our stocks, I have plotted the volatility of each. The performance of SPY and QTEC are highly similar because they also have high correlation. We see SHY also has a similar volatility but on a smaller scale because it deals with low risk treasury bonds. The hedgefund ETF RPAR has a very different performance trend from SPY, QTEC, and SHY. And our most volatile stock is the real estate ETF FPRO. 

 - SPY high performing
 - FPRO high risk, highest volatility
 - SHY low risk, high performing, lower return
 - QTEC high performing
 - RPAR high risk, lower return
```{r, echo=FALSE}
# Volatility check
plot(ClCl(SPYa), type='l')
plot(ClCl(FPROa), type='l')
plot(ClCl(SHYa), type='l')
plot(ClCl(QTECa), type='l')
plot(ClCl(RPARa), type='l')
```

## Portfolio 1: equal distribution among all stocks (.2)

 - SPY (.2)
 - FPRO (.2)
 - SHY (.2)
 - QTEC (.2)
 - RPAR (.2)

```{r, echo=FALSE}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine = rbind) %do% {
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  total_wealth = initial_wealth
  holdings = total_wealth * weights
  n_days = 20
  wealthtracker = rep(0, n_days)
  
  for(today in 1:n_days){
    return_today = resample(returns, 1, orig.ids=FALSE)
    holdings = holdings * (1 + return_today)
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    
    # Rebalancing
    holdings = total_wealth * weights
  }
  
  wealthtracker
}
head(sim1)
hist(sim1[,n_days], 50)
plot(density(sim1[,n_days]))
# Profit/loss
hist(sim1[,n_days]- initial_wealth, breaks=30)
conf_5Per = confint(sim1[,n_days]- initial_wealth, level = 0.90)$'5%'
cat('\nAverage return of investment after 20 days', mean(sim1[,n_days]), "\n")
cat('\n5% VaR for equal portfolio-',conf_5Per, "\n")
```

## Portfolio 2: low risk - put 50% of our equity in SHY, the high performing, low risk & low return treasury bond ETF. 20% in SPY an QTEC our highest returning ETF's. And 5% in the lower performing and higher risk ETF's FPRO and RPAR.
 
 - SPY (.2)
 - FPRO (.05)
 - SHY (.5)
 - QTEC (.2)
 - RPAR (.05)
 
```{r, echo=FALSE}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine = rbind) %do% {
  weights = c(0.2, 0.05, 0.5, 0.2, 0.05)
  total_wealth = initial_wealth
  holdings = total_wealth * weights
  n_days = 20
  wealthtracker = rep(0, n_days)
  
  for(today in 1:n_days){
    return_today = resample(returns, 1, orig.ids=FALSE)
    holdings = holdings * (1 + return_today)
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    
    # Rebalancing
    holdings = total_wealth * weights
  }
  
  wealthtracker
}
head(sim1)
hist(sim1[,n_days], 50)
plot(density(sim1[,n_days]))
# Profit/loss
hist(sim1[,n_days]- initial_wealth, breaks=30)
conf_5Per = confint(sim1[,n_days]- initial_wealth, level = 0.90)$'5%'
cat('\nAverage return of investment after 20 days', mean(sim1[,n_days]), "\n")
cat('\n5% VaR for safe portfolio-',conf_5Per, "\n")
```
## Portfolio 3: high risk - 70% of equity will go into the highly volatile ETF's FPRO and RPAR, while only 10% will go into our 'safe' investment SHY and 20% going into our large and high performing ETF's SPY and QTEC. 
 
 - SPY (.1)
 - FPRO (.35)
 - SHY (.1)
 - QTEC (.1)
 - RPAR (.35)

```{r, echo=FALSE}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine = rbind) %do% {
  weights = c(0.1, 0.35, 0.1, 0.1, 0.35)
  total_wealth = initial_wealth
  holdings = total_wealth * weights
  n_days = 20
  wealthtracker = rep(0, n_days)
  
  for(today in 1:n_days){
    return_today = resample(returns, 1, orig.ids=FALSE)
    holdings = holdings * (1 + return_today)
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    
    # Rebalancing
    holdings = total_wealth * weights
  }
  
  wealthtracker
}
head(sim1)
hist(sim1[,n_days], 50)
plot(density(sim1[,n_days]))
# Profit/loss
hist(sim1[,n_days]- initial_wealth, breaks=30)
conf_5Per = confint(sim1[,n_days]- initial_wealth, level = 0.90)$'5%'
cat('\nAverage return of investment after 20 days', mean(sim1[,n_days]), "\n")
cat('\n5% VaR for risky portfolio-',conf_5Per, "\n")
```
Conclusion: 
Our returns on all 3 portfolios stayed roughly the same, with the risky portfolio having the highest return on investment after 20 days. 

The highest 5% VaR ( -3112.753 ) was on our equally distributed portfolio and not the high risk portfolio which I thought was an interesting result. If the trading period was longer than 20 days I think we would have seen much larger differences in the performances of the portfolios. 


# 4: Market Segmentation
This was data collected in the course of a market-research study using followers of the Twitter account of a large consumer brand that shall remain nameless---let's call it "NutrientH20" just to have a label. The goal here was for NutrientH20 to understand its social-media audience a little bit better, so that it could hone its messaging a little more sharply.
```{r, echo=FALSE}
#clearing history for new question 
rm(list = ls())
```

## loading libraries and dataset
```{r q4 echo = FALSE}
#libraries used 
library(cluster)
library(FactoMineR)
library("factoextra")
library(dplyr)
library(tidyverse)


#reading in social_marketing data
library(readr)
social_marketing <- read_csv("C:/Users/Sophia Scott/Documents/a.STA 380 MACHINE LEARNING/PART 2/social_marketing.csv")
View(social_marketing)
#PreProcessing 
mkt = social_marketing[social_marketing$spam == 0 & social_marketing$adult == 0, ]
mkt = mkt %>%
  select(-chatter, -photo_sharing, - uncategorized)
View(mkt)
clean = mkt[,2:32]

```
The preprocessing of the data: 
the rows where "spam" or "adult" was dedected were removed to help focus on the true audience of the brand instead of data from Bots. 
the 'columns chatter', 'photo_sharing', and 'uncategorized' were removed so brand could derive useful insights from more unique information 



## correlation of variables 
```{r q4 echo = FALSE}
#correlation of variables 
# Loading
library(ggcorrplot)
cormat <- round(cor(clean), 2)
ggcorrplot(cormat, hc.order = TRUE, type = 'lower', outline.color = 'white', sig.level=0.05, lab_size = 4.5, p.mat = NULL, insig = c("pch", "blank"), pch = 1, pch.col = "black", pch.cex =5, tl.cex = 7) 
```
The Correlation plot shows low correlation between the majority of the variables 
where there is similarity is between a few clusters mostly between the columns of parenting, religion, sports fandom, food, family and school


## Initial PCA model 
```{r}
pr_out <-prcomp(clean, center = TRUE, scale = TRUE) 
summary(pr_out)
plot(pr_out, main = 'PCA', xlab = "Principal Components", type = 'b')

pr_var <-  pr_out$sdev ^ 2

pve <- pr_var / sum(pr_var)
plot(pve, main = 'Variance explained by PCA', xlab = "Principal Component",
ylab = "Proportion of Variance Explained",
type = "b")

loadings_summary = pr_out$rotation %>%
as.data.frame() %>%
rownames_to_column('Interest')
loadings_summary %>%
select(Interest, PC1) %>%
arrange(desc(PC1))

rot_loading <- varimax(pr_out$rotation[, 1:5])
rot_loading
```
here we can see that the first principal components explains about 13% of the variation on the data. this is quite low indicating the need to include multiple components 
The graph
The Variance Explained by PCA graph gives a better look into how many Principal Componenets are useful to include in our analysis until the utility of an additional variable plummets. Here we can see that around 10 PCAs hold the most useful information. 

## Further PCA Analysis: specific factors 
```{r q4 echo = FALSE}

#not including chatter to help get insighful information 
PCA.twit = prcomp(clean, scale=TRUE)
PCA.twit
colorplot = fviz_pca_var(PCA.twit, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )
colorplot

```
The variables - PCA grap: A higher cos2 (more red in color) signifies that the variable has higher quality 
The placement of the variables on the map explain how the variables are connected to the best perfroming principal components 

From this graph we can infer:

 - sports_fandom
 - religion
 - parenting 
 - food 
 - school 
 - family 
are related and hold significant information to the model

 - cooking 
 - fashion 
 - health nutrition 
 - personal fitness 
 - outdoors 
are related and hold (2nd best) significant information to the model 

```{r q4 echo = FALSE}

var <- get_pca_var(PCA.twit)
var

eig.val.twitter <- get_eigenvalue(PCA.twit)
eig.val.twitter
fviz_eig(PCA.twitter, addlabels = TRUE, ylim = c(0, 30))
head(var$contrib, 15)

# Contributions of variables to PC1
fviz_contrib(PCA.twitter, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(PCA.twitter, choice = "var", axes = 2, top = 10)

```
Dimension 1 and 2 have stayed consistent in which variables are included in holding the most information 
the eigen value is used to show which dimensions hold the most importance where a common rule of any value aboe 1 is a good idicator that it holds importance to explaining the dataset
around the dimension of 8-9 the usefulness of the dimensions begins to tapper off 

## Kmeans Cluster Analysis with the dataset values
```{r}

#cluster analysis 
twitter.k5 = kmeans(clean, centers = 5, nstart = 25)
twitter.k5
twitter.k9 = kmeans(clean, centers = 9, nstart = 25)
twitter.k9


plot_k9 = clusplot(twitter.ai, twitter.k9$cluster, color=T, shade=F, labels=4, lines=0, main="K-means cluster plot")
plot_k9
```
5 clusters helps explain 40% of the data while 9 clusters helps explain 52% of the data
However, the clusters are incredibly overlapped using component 1 and 2 (PCA 1 and 2)

#K Means CLuster Analysis with PCA 
```{r}
# Center and scale the data
install.packages('LICORS')
library(LICORS) 

X = as.data.frame(PCA.twit$x[, 1:11])
View(X)
X_scaled = scale(X, center=TRUE, scale=TRUE)

mu = attr(X_scaled,"scaled:center")
sigma = attr(X_scaled,"scaled:scale")


# Run k-means with 6 clusters and 25 starts
clust1 = kmeans(X, 6, nstart=25)
clust1
plot_k1 = clusplot(X,clust1$cluster, color=T, shade=F, labels=6, lines=0, main="K-means cluster plot")

#the centers for cluster analysis 1 
clust1$center[1,]*sigma + mu

# Using kmeans++ initialization
clust2 = kmeanspp(X, k=9, nstart=25)
clust2
plot_k2 = clusplot(X_scaled,clust2$cluster, color=T, shade=F, labels=8, lines=0, main="K-means cluster plot")

# A few plots with cluster membership shown
qplot(PC1, PC3, data=X, color=factor(clust1$cluster))
qplot(PC2, PC4, data=X, color=factor(clust1$cluster))
qplot(PC7, PC5, data=X, color=factor(clust1$cluster))
qplot(PC3, PC6, data=X, color=factor(clust1$cluster))

```
The new cluster plots are no longer fanned out but the data is still very much grouped together 
4 plots are then run comparing the points with different PCAs as the axis, successfully splitting the clusters was not possible but important information can still be obtained 
## Conclusion: 
NutrientH20 has 2 major consumer bases that can be targeted: 

First Consumer segment: 
"The American Traditional Family" 
 - sports_fandom
 - religion
 - parenting 
 - food 
 - school 
 - family 
 
Second COnsumer Segment:  
"The Active and Current Consumer" 
 - cooking 
 - fashion 
 - health nutrition 
 - personal fitness 
 - outdoors 

these segment names are chosen due to the overall summation of the variables that consistently held the most unique insights. 


# 5: Author Attribution 
Revisit the Reuters C50 corpus that we explored in class. Your task is to build the best model you can, using any combination of tools you see fit, for predicting the author of an article on the basis of that article's textual content. Describe clearly what models you are using, how you constructed features, and so forth. Yes, this is a supervised learning task, but it potentially draws on a lot of what you know about unsupervised learning, since constructing features for a document might involve dimensionality reduction.
```{r q5, echo=FALSE}
#clearing history for new question 
rm(list = ls())
```

## Reading in C50 train files and cleaning them
```{r q5 echo = FALSE}
install.packages('tm')
install.packages('proxy')
library(tm)
library(tidyverse)
library(slam)
library(proxy)

# Remember to source in the "reader" wrapper function
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }

#reading in the c50 training docs 
author_dirs = Sys.glob('C:/Users/Sophia Scott/Documents/a.STA 380 MACHINE LEARNING/PART 2/ReutersC50/C50train/*')

#reading in all the text files for each author 
file_list = NULL
labels = NULL
for(author in author_dirs) {
	author_name = substring(author, first=50)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list = append(file_list, files_to_add)
	labels = append(labels, rep(author_name, length(files_to_add)))
}


#cleaning the names to show all file names pretty 
readerPlain <- function(fname){ readPlain(elem=list(content=readLines(fname)), 
                            id=fname, language='en')}
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))


#create a text mining corpus 
documents_raw = Corpus(VectorSource(all_docs))

#preprocessing of data 
my_documents = documents_raw %>%
  tm_map(content_transformer(tolower))  %>%            
  # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        
  # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%   
  # remove punctuation
  tm_map(content_transformer(stripWhitespace))         
  # remove excess white-space

#remove stop words
#apply larger list of SMART stop words to remove 
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("SMART"))

#the x matrix where all rows correspond to the diff documents 
# y matrix corresponds to all words in dictionary 
DTM_train = DocumentTermMatrix(my_documents)
DTM_train # some basic summary statistics

#SPARCITY = how many 0 componenets you have 92% are 0 
# very sparce matrix 

class(DTM_train)  # a special kind of sparse matrix format

## You can inspect its entries...
inspect(DTM_train[1:10,1:20])
# visual of x matrix 
findFreqTerms(DTM_train, 50)
#most frequently used terms 

DTM_train = removeSparseTerms(DTM_train, 0.95)


# construct TF IDF weights -- might be useful if we wanted to use these
# as features in a predictive model
tfidf_train = weightTfIdf(DTM_train)

# Now a dense matrix
train_matrix = as.matrix(tfidf_train)
```


## Reading in the C50 test files and cleaning them 
```{r q5 echo = FALSE}
#reading in testing file information
test=Sys.glob('C:/Users/Sophia Scott/Documents/a.STA 380 MACHINE LEARNING/PART 2/ReutersC50/C50test/*')

file_list2 = NULL
labels2 = NULL
for(author in test) {
	author_name2 = substring(author, first=50)
	files_to_add2 = Sys.glob(paste0(author, '/*.txt'))
	file_list2 = append(file_list2, files_to_add2)
	labels2 = append(labels2, rep(author_name2, length(files_to_add2)))
}


#cleaning the names to show all file names pretty 
all_docs2 = lapply(file_list2, readerPlain) 
names(all_docs2) = file_list2
names(all_docs2) = sub('.txt', '', names(all_docs2))


#create a text mining corpus 
documents_raw2 = Corpus(VectorSource(all_docs2))

#preprocessing of data 
test_docs = documents_raw2 %>%
  tm_map(content_transformer(tolower))  %>%            
  # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        
  # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%   
  # remove punctuation
  tm_map(content_transformer(stripWhitespace))         
  # remove excess white-space


#apply larger list of SMART stop words to remove 
test_docs = tm_map(test_docs, content_transformer(removeWords), stopwords("SMART"))

#the x matrix where all rows correspond to the diff documents 
# y matrix corresponds to all words in dictionary 
DTM_test = DocumentTermMatrix(test_docs,list(dictionary=colnames(DTM_train)))

tfidf_test = weightTfIdf(DTM_test)
test_matrix<-as.matrix(tfidf_test)
```


## Running PCA on the training data
```{r q5 echo = FALSE}
##PCA
train_matrix_1<-train_matrix[,which(colSums(train_matrix) != 0)] 
test_matrix_1<-test_matrix[,which(colSums(test_matrix) != 0)]

test_matrix_1 = test_matrix_1[,intersect(colnames(test_matrix_1),colnames(train_matrix_1))]
train_matrix_1 = train_matrix_1[,intersect(colnames(test_matrix_1),colnames(train_matrix_1))]
PCA_train = prcomp(train_matrix_1,scale=TRUE)
PCA_train
PCA_test = prcomp(test_matrix_1,scale=TRUE)

plot(PCA_train,type='line') 

#variance and St_dev of OCA
PCA_variance = (PCA_train$sdev)^2 
prop_var = PCA_variance / sum(PCA_variance)
plot(prop_var, xlab= 'PCA', ylab = "proportion of var explained", type = 'b' )
plot(cumsum(prop_var),xlab= 'PCA', ylab = "proportion of cumsum var explained")
segments(x0=300, y0=0, x1= 300, y1=.8) # segments from x axis to points
segments(x0=0, y0=.8, x1=300, y1=.8) 

sort(PCA_train$x, decreasing = TRUE)
View(PCA_train$x)

#Creating the training and test sets 
train_class = data.frame(PCA_train$x[,1:300])
train_class['author']=labels
tr_load = PCA_train$rotation[,1:300]
test_class.pre <- scale(test_matrix_1) %*% tr_load
test_class <- as.data.frame(test_class.pre)
test_class['author']=labels2
View(train_class)
View(test_class)

```
The PCA helps show how many components are useful to refer to in our training data
The graph 'proportion of cumsum var explained' or the variance explained by each PCA factor is used to pinpoint 80% accuracy and how many componenets it will tae to acheive such result 

## Running Naive Bayes and Random Forests to Compare Prediction Accuracy
```{r q5 echo = FALSE}
#naive bayes 
install.packages("e1071", dep = TRUE) 
library('e1071')

naive=naiveBayes(Authors~.,data=pca_df)
pred_naive=predict(naive,test_class)

library(caret)
predicted_nb=pred_naive
actual_nb=as.factor(test_class$author)
temp_nb<-as.data.frame(cbind(actual_nb,predicted_nb))
temp_nb$flag<-ifelse(temp_nb$actual_nb==temp_nb$predicted_nb,1,0)

correct = sum(temp_nb$flag)
accuracyNB = sum(temp_nb$flag)*100/nrow(temp_nb)
print("the accuracy for Naive Bayes:")
accuracyNB


#Random Forest
library(randomForest)

rf = randomForest(Authors~.,data=pca_df)
CM = table(predict(rf), pca_df$Author)
accuracyRF = (sum(diag(CM)))/sum(CM)
print("the accuracy for Random Forests:") 
accuracyRF


```
The random forests prediction accuracy is much higher than Naive Bayes, therefore when predicting for authors based on the textual content of these files, Random forests is a better option. 




# Association rule mining
goals: find interesting association rules for the baskets, pick thresholds for lift & confidence

Increasing the confidence level gives you stronger rules (this WILL happen)
Increasing/Decreasing the maxlen gives you different sized groupings of items purchased

## 1. the set up 

Total number of items purchased: 43367 (rows * columns * density)
Most common items: whole milk, other vegetables, rolls/buns, soda, yogurt
```{r, echo=FALSE}
library(arules)
library(arulesViz)
library(tidyverse)
library(readxl)
library(knitr)
library(ggplot2)
library(lubridate)
library(plyr)
library(dplyr)

#set up the grocery file for basket analysis
groceries = scan("groceries.txt", what = "", sep = "\n") #9835 items 
groceries = strsplit(groceries, ",")
transactions = as(groceries, "transactions")
summary(transactions)

#number of total items purchased
 9835 *  169 * 0.02609146
 
 itemFrequencyPlot(transactions, topN = 20, type = 'absolute', col = 'coral',  main = 'Absolute Item Frequency Plot')
```
2. Playing around with different rules for the transaction list 
The summary function of association.rules shows us that confidence can range from .8 to 1, lift ranges from 3.131 to 11.235, and support ranges from .001 to .003. The following models will play around with these different levels to see if we can find the most common rules associated with the shopping basket analysis. 

We used interactive plots for our analysis and added non-interactive plots for each model tried in order to ensure proper printing output when knitting to a PDF file

  1. minimum support of 0.001 and minimum confidence of .8, maximum of 10 items per rule
This first run resulted in 410 rules, with whole milk showing up quite a few times as it is our most common item in this dataset. 
```{r, echo=FALSE}
association.rules <- apriori(transactions, parameter = list(supp=0.001, conf=0.8,maxlen=10))
summary(association.rules)

inspect(association.rules[1:10])

plot(head(association.rules,10,by='lift'), method='graph')

top10Rules <- head(association.rules, n = 10, by = "confidence")
plot(top10Rules, method = "graph",  engine = "htmlwidget")
top10Lift <- head(association.rules, n = 10, by = "lift")
plot(top10Rules, method = "graph",  engine = "htmlwidget")
```

## 2. increasing CI to .9 
      support of 0.001 and minimum confidence of .9, still limiting rule size to 10 
      
By increasing our confidence level we now have a set of 129 rules. 
```{r, echo=FALSE}
association.rules <- apriori(transactions, parameter = list(supp=0.001, conf=0.9,maxlen=10))
summary(association.rules)

inspect(association.rules[1:10])

plot(head(association.rules,10,by='lift'), method='graph')

top10Rules <- head(association.rules, n = 10, by = "confidence")
plot(top10Rules, method = "graph",  engine = "htmlwidget")
top10Lift <- head(association.rules, n = 10, by = "lift")
plot(top10Rules, method = "graph",  engine = "htmlwidget")

subset.rules <- which(colSums(is.subset(association.rules, association.rules)) > 1) #20 rules repeat

subset.association.rules. <- association.rules[-subset.rules] # remove subset rules.
```

## 3. Decreasing maxlen to 3
```{r, echo=FALSE}
association.rules <- apriori(transactions, parameter = list(supp=0.001, conf=0.8,maxlen=3))
summary(association.rules)

inspect(association.rules[1:10])

plot(head(association.rules,10,by='lift'), method='graph')

top10Rules <- head(association.rules, n = 10, by = "confidence")
plot(top10Rules, method = "graph",  engine = "htmlwidget")
top10Lift <- head(association.rules, n = 10, by = "lift")
plot(top10Rules, method = "graph",  engine = "htmlwidget")
```

## 4. increasing Confidence to 100% 
      support of 0.001 and  confidence of 100%, still limiting rule size to 10 
      
      Resulted in a set of 28 rules
```{r, echo=FALSE}
association.rules <- apriori(transactions, parameter = list(supp=0.001, conf=1,maxlen=10))
summary(association.rules)

plot(head(association.rules,10,by='lift'), method='graph')

inspect(association.rules[1:10])
top10Rules <- head(association.rules, n = 10, by = "confidence")
plot(top10Rules, method = "graph",  engine = "htmlwidget")
top10Lift <- head(association.rules, n = 10, by = "lift")
plot(top10Rules, method = "graph",  engine = "htmlwidget")
```

## 5. Increase support to .002, decrease confidence to .8 
  
  Resulted in a set of 11 rules. 

```{r, echo=FALSE}
association.rules <- apriori(transactions, parameter = list(supp=0.002, conf=.8,maxlen=10))
summary(association.rules)

inspect(association.rules[1:10])

plot(head(association.rules,10,by='lift'), method='graph')

top10Rules <- head(association.rules, n = 10, by = "confidence")
plot(top10Rules, method = "graph",  engine = "htmlwidget")
top10Lift <- head(association.rules, n = 10, by = "lift")
plot(top10Rules, method = "graph",  engine = "htmlwidget")
```

Conclusion: 

While changes the support level, confidence level, and length of rules changes the ammount of rules we obtains, 3 main trends stayed consistent throughout all models. 
1. Whole milk is the most common product purchased, and most commonly seen in all of our rules. It is most often bought in addition to other dairy products and also what seemed like to me as 'recipe ingredients' (i.e. eggs, flour, sugar, herbs, etc.)
2. When purchasing one type of alcohol, consumers usually purchase other types as well. Bottled beered was associated with liquor and blush wine in multiple models. 
3. People who buy one vegetable usually buy other types of fruit and vegetables as well. 

Stores can use associate rule mining to determine consumer buying patterns, which items sell the best, what areas of the store to focus on (always keeping the milk section stocked, having a wide array of fruits & vegetables), and ways to improve their store layout (targeted end caps near the milk section, putting popular wine and beer options next to eachother on the shelf. )